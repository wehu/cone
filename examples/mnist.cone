module examples.mnist

import data.tensor
import ml.nn.conv2d
import ml.nn.maxpool2d
import ml.nn.softmax
import ml.nn.relu
import ml.nn.dropout
import ml.nn.fc
import ml.optimizer.loss
import ml.vision.datasets

alias mnist_weights = (tensor<f32, @[32,1,3,3]>,
                       tensor<f32, @[64,32,3,3]>,
                       tensor<f32, @[128,9216]>,
                       tensor<f32, @[128]>,
                       tensor<f32, @[10,128]>,
                       tensor<f32, @[10]>)

fun mnist_model(input:tensor<f32, @[1,1,28,28]>,
                weights:mnist_weights) : tensor<f32, @[1,10]> {
    val (conv1, conv2, fcw1, fcb1, fcw2, fcb2) = weights
    val x = conv2d(input, conv1)
    val x = relu(x)
    val x = conv2d(x, conv2)
    val x = relu(x)
    val x = maxpool2d<2,2>(x)
    val x = dropout(x, 0.25)
    val x = reshape<@[1, 9216]>(x)
    val x = fc(x, fcw1, fcb1)
    val x = relu(x)
    val x = dropout(x, 0.5)
    val x = fc(x, fcw2, fcb2)
    val x = log_softmax(x,1)
    x
}

fun mnist_train(input:tensor<f32, @[1,1,28,28]>,
                target:tensor<i32, @[1]>,
                weights:mnist_weights) : f32 {
    val x = mnist_model(input, weights)
    nll_loss(x, target)
}

diff mnist_model wrt (weights) = auto
diff mnist_train wrt (weights) = auto

fun main() : io unit {
    val ds = mnist("./data", true)
    var weights = (full<@[32,1,3,3]>(1.0, []),
                   full<@[64,32,3,3]>(1.0, []),
                   full<@[128,9216]>(1.0, []),
                   full<@[128]>(1.0, []),
                   full<@[10,128]>(1.0, []),
                   full<@[10]>(1.0, []))
    val rate = 0.1
    foreach(ds, fn(d : (tensor<f32, @[1,28,28]>, i32), i : i32) : io unit {
        val (data, target) = d
        val data = reshape<@[1,1,28,28]>(data)
        val target = full<@[1]>(target, [])
        val (conv1_grad, conv2_grad, fcw1_grad, fcb1_grad, fcw2_grad, fcb2_grad) = 
            diff mnist_train(data, target, weights, 1.0)
        weights = weights - (conv1_grad * full<@[32,1,3,3]>(rate, []),
                             conv2_grad * full<@[64,32,3,3]>(rate, []),
                             fcw1_grad * full<@[128,9216]>(rate, []),
                             fcb1_grad * full<@[128]>(rate, []),
                             fcw2_grad * full<@[10,128]>(rate, []),
                             fcb2_grad * full<@[10]>(rate, []))
        val result = mnist_train(data, target, weights)
        print(target)
        print(result)
    })
    print("finished")
}