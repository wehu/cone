module examples.mnist

import data.tensor
import ml.nn.conv2d
import ml.nn.maxpool2d
import ml.nn.softmax
import ml.nn.relu
import ml.nn.dropout
import ml.nn.fc
import ml.optimizer.loss
import ml.vision.datasets

fun mnist_train(input:tensor<f32, @[1,1,28,28]>, \
                target:tensor<f32, @[1]>,         \
                conv1:tensor<f32, @[32,1,3,3]>,  \
	            conv2:tensor<f32, @[64,32,3,3]>, \
                fcw1:tensor<f32, @[128,9216]>,   \
                fcb1:tensor<f32, @[128]>,        \
                fcw2:tensor<f32, @[10,128]>,     \
                fcb2:tensor<f32, @[10]>) : f32 {
    val x = conv2d(input, conv1)
    val x = relu(x)
    val x = conv2d(x, conv2)
    val x = relu(x)
    val x = maxpool2d<2,2>(x)
    val x = dropout(x, 0.25)
    val x = reshape<@[1, 9216]>(x)
    val x = fc(x, fcw1, fcb1)
    val x = relu(x)
    val x = dropout(x, 0.5)
    val x = fc(x, fcw2, fcb2)
    val x = log_softmax(x,1)
    nll_loss(x, target)
}

diff mnist_train wrt (conv1, conv2, fcw1, fcb1, fcw2, fcb2) = auto

fun main() : io unit {
    val ds = mnist("./data", true)
    var conv1 = full<@[32,1,3,3]>(1.0, [])
    var conv2 = full<@[64,32,3,3]>(1.0, [])
    var fcw1 = full<@[128,9216]>(1.0, [])
    var fcb1 = full<@[128]>(1.0, [])
    var fcw2 = full<@[10,128]>(1.0, [])
    var fcb2 = full<@[10]>(1.0, [])
    foreach(ds, fn(d : (tensor<f32, @[1,28,28]>, f32), i : i32) : io unit {
        if (i < 1) {
            val (data, target) = d
            val data = reshape<@[1,1,28,28]>(data)
            val target = full<@[1]>(target, [])
            val (conv1_grad, conv2_grad, fcw1_grad, fcb1_grad, fcw2_grad, fcb2_grad) = \
                  diff mnist_train(data, target, conv1, conv2, fcw1, fcb1, fcw2, fcb2, 1.0)
            print(conv2_grad)
        } else {
            unit
        }
    })
}